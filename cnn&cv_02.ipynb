{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 2 : Building a Convolutional Neural Network (CNN)\n",
    "CNN's are also known as ConvNets.\n",
    "\n",
    "They are known for their ability to find patterns in visual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "0.15.0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "# Import torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt \n",
    "import tqdm\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=datasets.FashionMNIST(\n",
    "    root=\"data\", #where to download data to\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(), #How do we want to transform the data ?\n",
    "    target_transform=None  #How do we want to transform the labels/targets ?\n",
    ")\n",
    "\n",
    "test_data=datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "len(train_data),len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image,label=train_data[0]\n",
    "class_names=train_data.classes\n",
    "class_to_idx = train_data.class_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x2121a891520>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x2121a8a2d60>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setup the batch size hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "#Turn datasets into iterables ( batches )\n",
    "train_dataloader=DataLoader(dataset=train_data,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True)\n",
    "test_dataloader=DataLoader(dataset=test_data,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           shuffle=False)\n",
    "\n",
    "train_dataloader,test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model:torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn:torch.nn.Module,\n",
    "               accuracy_fn):\n",
    "    \"\"\" Returns a dictionary containing the results of model predicting on data_loader\"\"\"\n",
    "    loss,acc=0,0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X,y in tqdm(data_loader):\n",
    "            \n",
    "            #make preds\n",
    "            y_pred=model(X)\n",
    "\n",
    "            #Accumulate the loss and acc values per batch\n",
    "            loss += loss_fn(y_pred,y)\n",
    "            acc += accuracy_fn(y_true=y,\n",
    "                               y_pred=y_pred.argmax(dim=1))\n",
    "        # Scale loss and acc to find the avarage loss/ acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "\n",
    "    return {\"model_name\": model.__class__.__name__,#only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn\n",
    "def train_step(model:torch.nn.Module,\n",
    "               data_loader:torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device:torch.device=device):\n",
    "    \"\"\"Performs a training with model trying to learn on data_loader.\"\"\"\n",
    "    train_loss,train_acc=0,0\n",
    "    \n",
    "    #Put model into training mode\n",
    "    model.train()\n",
    "\n",
    "    # Add a loop to loop through training batches X-> image, y->label(target)\n",
    "    for batch , (X,y) in enumerate(data_loader):\n",
    "        \n",
    "        #Put data on target device\n",
    "        X,y=X.to(device),y.to(device)\n",
    "        \n",
    "        #1. Forward pass (outputs the raw logits from the model)\n",
    "        y_pred=model(X)\n",
    "\n",
    "        #2. Calculate the loss\n",
    "        loss=loss_fn(y_pred,y)\n",
    "        train_loss +=loss #accumulate train loss\n",
    "        train_acc+=accuracy_fn(y_true=y,\n",
    "                               y_pred=y_pred.argmax(dim=1)) #go from logits->prediction labels\n",
    "        \n",
    "        #3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        #5. Optimizer\n",
    "        optimizer.step() \n",
    "\n",
    "    train_loss/=len(data_loader)\n",
    "    train_acc/=len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model:torch.nn.Module,\n",
    "              data_loader:torch.utils.data.DataLoader,\n",
    "              loss_fn:torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device:torch.device=device):\n",
    "    \"\"\"Performs a testing loop step on model going over data_loader.\"\"\"\n",
    "    test_loss,test_acc=0,0\n",
    "\n",
    "    #Put model into eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for X,y in data_loader:\n",
    "            #Data into device\n",
    "            X,y=X.to(device),y.to(device)\n",
    "            \n",
    "            #1. Forward pass\n",
    "            test_pred=model(X)\n",
    "\n",
    "            #2. Calculate loss and acc(accumulatively)\n",
    "            test_loss+=loss_fn(test_pred,y)\n",
    "            test_acc+=accuracy_fn(y_true=y,\n",
    "                                  y_pred=test_pred.argmax(dim=1))\n",
    "            \n",
    "        # Calculate the test loss/acc average per patch\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        print(f\"\\nTrain loss: {train_loss:.4f} | Test loss: {test_loss:.4f}, Test acc {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([1, 28, 28])\n",
      "Label: 7, label size: torch.Size([])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_features_batch,train_labels_batch=next(iter(train_dataloader))\n",
    "train_features_batch.shape,train_labels_batch.shape\n",
    "random_idx=torch.randint(0,len(train_features_batch),size=[1]).item()\n",
    "img,label=train_features_batch[random_idx],train_labels_batch[random_idx]\n",
    "plt.imshow(img.squeeze(),cmap=\"gray\")\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {label}, label size: {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class FashionMNISTModelV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture that replicates TinyVGG\n",
    "    model from CNN explainer site.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape: int,\n",
    "                 hidden_units: int,\n",
    "                 output_shape: int):\n",
    "        super().__init__()\n",
    "        self.conv_block_1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1), #hyperparameters of Conv2d\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)#kernel size can also be a tuple such as (2,2)\n",
    "\n",
    "        )\n",
    "        self.conv_block_2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.classifier_layer=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*0,\n",
    "                      out_features=output_shape)#out features -> class_names\n",
    "        )\n",
    "\n",
    "    def forward(self,x:torch.tensor):\n",
    "        x=self.conv_block_1(x)\n",
    "        print(x.shape)\n",
    "        x=self.conv_block_2(x)\n",
    "        print(x.shape)\n",
    "        x=self.classifier_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TRON PCH\\miniconda3\\envs\\ai_env2\\lib\\site-packages\\torch\\nn\\init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "#input shape is 1 beacuse we have 1 color channel\n",
    "model_2=FashionMNISTModelV2(input_shape=1,\n",
    "                            hidden_units=10,\n",
    "                            output_shape=len(class_names)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
