{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 2 : Building a Convolutional Neural Network (CNN)\n",
    "CNN's are also known as ConvNets.\n",
    "\n",
    "They are known for their ability to find patterns in visual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "0.15.0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "# Import torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt \n",
    "import tqdm\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=datasets.FashionMNIST(\n",
    "    root=\"data\", #where to download data to\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(), #How do we want to transform the data ?\n",
    "    target_transform=None  #How do we want to transform the labels/targets ?\n",
    ")\n",
    "\n",
    "test_data=datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "len(train_data),len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image,label=train_data[0]\n",
    "class_names=train_data.classes\n",
    "class_to_idx = train_data.class_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x244b1b3e520>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x244bdc16ee0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setup the batch size hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "#Turn datasets into iterables ( batches )\n",
    "train_dataloader=DataLoader(dataset=train_data,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True)\n",
    "test_dataloader=DataLoader(dataset=test_data,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           shuffle=False)\n",
    "\n",
    "train_dataloader,test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model:torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn:torch.nn.Module,\n",
    "               accuracy_fn):\n",
    "    \"\"\" Returns a dictionary containing the results of model predicting on data_loader\"\"\"\n",
    "    loss,acc=0,0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X,y in tqdm(data_loader):\n",
    "            \n",
    "            #make preds\n",
    "            y_pred=model(X)\n",
    "\n",
    "            #Accumulate the loss and acc values per batch\n",
    "            loss += loss_fn(y_pred,y)\n",
    "            acc += accuracy_fn(y_true=y,\n",
    "                               y_pred=y_pred.argmax(dim=1))\n",
    "        # Scale loss and acc to find the avarage loss/ acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "\n",
    "    return {\"model_name\": model.__class__.__name__,#only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn\n",
    "def train_step(model:torch.nn.Module,\n",
    "               data_loader:torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device:torch.device=device):\n",
    "    \"\"\"Performs a training with model trying to learn on data_loader.\"\"\"\n",
    "    train_loss,train_acc=0,0\n",
    "    \n",
    "    #Put model into training mode\n",
    "    model.train()\n",
    "\n",
    "    # Add a loop to loop through training batches X-> image, y->label(target)\n",
    "    for batch , (X,y) in enumerate(data_loader):\n",
    "        \n",
    "        #Put data on target device\n",
    "        X,y=X.to(device),y.to(device)\n",
    "        \n",
    "        #1. Forward pass (outputs the raw logits from the model)\n",
    "        y_pred=model(X)\n",
    "\n",
    "        #2. Calculate the loss\n",
    "        loss=loss_fn(y_pred,y)\n",
    "        train_loss +=loss #accumulate train loss\n",
    "        train_acc+=accuracy_fn(y_true=y,\n",
    "                               y_pred=y_pred.argmax(dim=1)) #go from logits->prediction labels\n",
    "        \n",
    "        #3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        #5. Optimizer\n",
    "        optimizer.step() \n",
    "\n",
    "    train_loss/=len(data_loader)\n",
    "    train_acc/=len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model:torch.nn.Module,\n",
    "              data_loader:torch.utils.data.DataLoader,\n",
    "              loss_fn:torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device:torch.device=device):\n",
    "    \"\"\"Performs a testing loop step on model going over data_loader.\"\"\"\n",
    "    test_loss,test_acc=0,0\n",
    "\n",
    "    #Put model into eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for X,y in data_loader:\n",
    "            #Data into device\n",
    "            X,y=X.to(device),y.to(device)\n",
    "            \n",
    "            #1. Forward pass\n",
    "            test_pred=model(X)\n",
    "\n",
    "            #2. Calculate loss and acc(accumulatively)\n",
    "            test_loss+=loss_fn(test_pred,y)\n",
    "            test_acc+=accuracy_fn(y_true=y,\n",
    "                                  y_pred=test_pred.argmax(dim=1))\n",
    "            \n",
    "        # Calculate the test loss/acc average per patch\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        print(f\"\\nTrain loss: {train_loss:.4f} | Test loss: {test_loss:.4f}, Test acc {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([1, 28, 28])\n",
      "Label: 3, label size: torch.Size([])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_features_batch,train_labels_batch=next(iter(train_dataloader))\n",
    "train_features_batch.shape,train_labels_batch.shape\n",
    "random_idx=torch.randint(0,len(train_features_batch),size=[1]).item()\n",
    "img,label=train_features_batch[random_idx],train_labels_batch[random_idx]\n",
    "plt.imshow(img.squeeze(),cmap=\"gray\")\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {label}, label size: {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class FashionMNISTModelV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture that replicates TinyVGG\n",
    "    model from CNN explainer site.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape: int,\n",
    "                 hidden_units: int,\n",
    "                 output_shape: int):\n",
    "        super().__init__()\n",
    "        self.conv_block_1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1), #hyperparameters of Conv2d\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)#kernel size can also be a tuple such as (2,2)\n",
    "\n",
    "        )\n",
    "        self.conv_block_2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.classifier_layer=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*0,\n",
    "                      out_features=output_shape)#out features -> class_names\n",
    "        )\n",
    "\n",
    "    def forward(self,x:torch.tensor):\n",
    "        x=self.conv_block_1(x)\n",
    "        print(x.shape)\n",
    "        x=self.conv_block_2(x)\n",
    "        print(x.shape)\n",
    "        x=self.classifier_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TRON PCH\\miniconda3\\envs\\ai_env2\\lib\\site-packages\\torch\\nn\\init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "#input shape is 1 beacuse we have 1 color channel\n",
    "model_2=FashionMNISTModelV2(input_shape=1,\n",
    "                            hidden_units=10,\n",
    "                            output_shape=len(class_names)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepping through nn.Conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 3, 64, 64])\n",
      "Single images shape: torch.Size([3, 64, 64])\n",
      "Test image:\n",
      " tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  1.8446, -1.1845,  1.3835],\n",
      "         [ 1.4451,  0.8564,  2.2181,  ...,  0.3399,  0.7200,  0.4114],\n",
      "         [ 1.9312,  1.0119, -1.4364,  ..., -0.5558,  0.7043,  0.7099],\n",
      "         ...,\n",
      "         [-0.5610, -0.4830,  0.4770,  ..., -0.2713, -0.9537, -0.6737],\n",
      "         [ 0.3076, -0.1277,  0.0366,  ..., -2.0060,  0.2824, -0.8111],\n",
      "         [-1.5486,  0.0485, -0.7712,  ..., -0.1403,  0.9416, -0.0118]],\n",
      "\n",
      "        [[-0.5197,  1.8524,  1.8365,  ...,  0.8935, -1.5114, -0.8515],\n",
      "         [ 2.0818,  1.0677, -1.4277,  ...,  1.6612, -2.6223, -0.4319],\n",
      "         [-0.1010, -0.4388, -1.9775,  ...,  0.2106,  0.2536, -0.7318],\n",
      "         ...,\n",
      "         [ 0.2779,  0.7342, -0.3736,  ..., -0.4601,  0.1815,  0.1850],\n",
      "         [ 0.7205, -0.2833,  0.0937,  ..., -0.1002, -2.3609,  2.2465],\n",
      "         [-1.3242, -0.1973,  0.2920,  ...,  0.5409,  0.6940,  1.8563]],\n",
      "\n",
      "        [[-0.7978,  1.0261,  1.1465,  ...,  1.2134,  0.9354, -0.0780],\n",
      "         [-1.4647, -1.9571,  0.1017,  ..., -1.9986, -0.7409,  0.7011],\n",
      "         [-1.3938,  0.8466, -1.7191,  ..., -1.1867,  0.1320,  0.3407],\n",
      "         ...,\n",
      "         [ 0.8206, -0.3745,  1.2499,  ..., -0.0676,  0.0385,  0.6335],\n",
      "         [-0.5589, -0.3393,  0.2347,  ...,  2.1181,  2.4569,  1.3083],\n",
      "         [-0.4092,  1.5199,  0.2401,  ..., -0.2558,  0.7870,  0.9924]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "images=torch.randn(size=(32,3,64,64))\n",
    "test_image=images[0]\n",
    "\n",
    "print(f\"Image batch shape: {images.shape}\")\n",
    "print(f\"Single images shape: {test_image.shape}\")\n",
    "print(f\"Test image:\\n {test_image}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 64])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "conv_layer=nn.Conv2d(in_channels=3,\n",
    "                     out_channels=64,\n",
    "                     kernel_size=3,\n",
    "                     stride=1,\n",
    "                     padding=1).to(device)\n",
    "\n",
    "conv_output=conv_layer(test_image.unsqueeze(0))\n",
    "conv_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test image original shape: torch.Size([3, 64, 64])\n",
      "Test image with unsqueezed dimension: torch.Size([1, 3, 64, 64])\n",
      "Shape after conv layer: torch.Size([1, 64, 64, 64])\n",
      "Shape after max pool : torch.Size([1, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "#Print out original image shape without unsqueezed dimension\n",
    "print(f\"Test image original shape: {test_image.shape}\")\n",
    "print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(0).shape}\")\n",
    "\n",
    "#Create a sample nn.MaxPool2d layer\n",
    "max_pool_layer=nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "#Pass data\n",
    "test_image_through_conv=conv_layer(test_image.unsqueeze(dim=0))\n",
    "print(f\"Shape after conv layer: {test_image_through_conv.shape}\")\n",
    "\n",
    "test_image_through_conv_and_max_pool=max_pool_layer(test_image_through_conv)\n",
    "print(f\"Shape after max pool : {test_image_through_conv_and_max_pool.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random tensor:\n",
      "tensor([[[[0.3367, 0.1288],\n",
      "          [0.2345, 0.2303]]]])\n",
      "Random tensor shape:torch.Size([1, 1, 2, 2])\n",
      "\n",
      "Max pool tensor:\n",
      "tensor([[[[0.3367]]]])\n",
      "Max pool tensor shape: torch.Size([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "random_tensor=torch.randn(size=(1,1,2,2))\n",
    "print(f\"\\nRandom tensor:\\n{random_tensor}\")\n",
    "print(f\"Random tensor shape:{random_tensor.shape}\")\n",
    "\n",
    "max_pool_layer=nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "max_pool_tensor=max_pool_layer(random_tensor)\n",
    "\n",
    "print(f\"\\nMax pool tensor:\\n{max_pool_tensor}\")\n",
    "print(f\"Max pool tensor shape: {max_pool_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
